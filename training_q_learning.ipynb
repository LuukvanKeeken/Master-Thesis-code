{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the code for \"Fine-tuning Deep Reinforcement Learning Policies with r-STDP for Domain Adaptation\" by Akl et al., which can be found at https://github.com/mahmoudakl/dsrl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN vs. DSQN for the CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import site\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "site.addsitedir('cartpole_stuff/src/')\n",
    "\n",
    "from datetime import date\n",
    "from model import QNetwork, DSNN\n",
    "from dqn_agent import Agent, ReplayBuffer\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from backpropamine_FFNN import FFNetwork\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'CartPole-v0'\n",
    "n_runs = 5\n",
    "n_evaluations = 100\n",
    "max_steps = 200\n",
    "num_episodes = 500\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory dqn_result_19_2023106 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'dqn_result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 0.999\n",
    "update_every = 4\n",
    "target_update_frequency = 100\n",
    "learning_rate = 0.001 # lr is 0.0001 for simple maze as default\n",
    "l2_coef = 0 # 0 is default in simple maze task\n",
    "replay_memory_size = 4*10**4\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN Hyperparameters\n",
    "simulation_time = 3\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "threshold = 0.2\n",
    "weight_scale = 1\n",
    "architecture = [4, 64, 64, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.load('cartpole_stuff/seeds/training_seeds.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 64] but got: [128, 64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/luuk/Desktop/testing/training_q_learning.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(policy_net\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m learning_rate, weight_decay \u001b[39m=\u001b[39m l2_coef) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env_name, policy_net, target_net, architecture, batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m               replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m               update_every, target_update_frequency, optimizer, learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m               num_episodes, max_steps, i_run, result_dir, seed, tau)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m smoothed_scores, scores, best_average, best_average_after \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain_agent()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m np\u001b[39m.\u001b[39msave(result_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/scores_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i_run), scores)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m np\u001b[39m.\u001b[39msave(result_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/smoothed_scores_DQN_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i_run), smoothed_scores)\n",
      "File \u001b[0;32m~/Desktop/testing/cartpole_stuff/src/dqn_agent.py:192\u001b[0m, in \u001b[0;36mAgent.train_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m    190\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_step_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 192\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_action(state, eps)\n\u001b[1;32m    193\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtwo_neuron:\n",
      "File \u001b[0;32m~/Desktop/testing/cartpole_stuff/src/dqn_agent.py:108\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(final_layer_values)\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[39m# return np.argmax(self.policy_net.forward(state.float())[0].cpu().data.numpy())\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     q_values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhebbian_traces \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net\u001b[39m.\u001b[39;49mforward(state\u001b[39m.\u001b[39;49mfloat(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhebbian_traces)\n\u001b[1;32m    109\u001b[0m     selected_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_values[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m selected_action\n",
      "File \u001b[0;32m~/Desktop/testing/backpropamine_FFNN.py:82\u001b[0m, in \u001b[0;36mFFNetwork.forward\u001b[0;34m(self, inputs, hebb)\u001b[0m\n\u001b[1;32m     66\u001b[0m pre_synaptic_activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layer_w(inputs))\n\u001b[1;32m     68\u001b[0m \u001b[39m# Batch size 1:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m#post_synaptic_activations = self.activ(torch.matmul((self.plastic_layer_w + \u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m#                            torch.mul(self.plastic_layer_alpha, hebb)), pre_synaptic_activations))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39m# pre_synaptic_activations unsqueezed gives BatchSize x 1 x num_neurons_layer_2,\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# which can be multiplied with BatchSize x num_neurons_layer_2 x num_neurons_layer_3\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m post_synaptic_activations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactiv(pre_synaptic_activations\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mbmm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplastic_layer_w \u001b[39m+\u001b[39;49m torch\u001b[39m.\u001b[39;49mmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplastic_layer_alpha, hebb))\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m))\n\u001b[1;32m     84\u001b[0m \u001b[39m# Output the Q-values. BatchSize x num_actions\u001b[39;00m\n\u001b[1;32m     85\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_value_output_layer_w(post_synaptic_activations)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 64] but got: [128, 64]."
     ]
    }
   ],
   "source": [
    "smoothed_scores_dqn_all = []\n",
    "dqn_completion_after = []\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = int(seeds[i_run])\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # policy_net = QNetwork(architecture, seed).to(device)\n",
    "    # target_net = QNetwork(architecture, seed).to(device)\n",
    "    policy_net = FFNetwork(4, 64, 64, 2, seed).to(device)\n",
    "    target_net = FFNetwork(4, 64, 64, 2, seed).to(device)\n",
    "\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr = learning_rate, weight_decay = l2_coef) \n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau)\n",
    "    \n",
    "    smoothed_scores, scores, best_average, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    dqn_completion_after.append(best_average_after)\n",
    "    smoothed_scores_dqn_all.append(smoothed_scores)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot scores of individual runs\n",
    "for i in range(len(smoothed_scores_dqn_all)):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(smoothed_scores_dqn_all[i])\n",
    "    plt.ylim(0, 250)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(result_dir + '/training_dqn_{}.png'.format(i), dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (mean)\n",
    "best_smoothed_scores_dqn = [smoothed_scores_dqn_all[0],\n",
    "                            smoothed_scores_dqn_all[1],\n",
    "                            smoothed_scores_dqn_all[2],\n",
    "                            smoothed_scores_dqn_all[3],\n",
    "                            smoothed_scores_dqn_all[4]]\n",
    "mean_smoothed_scores_dqn = np.mean(best_smoothed_scores_dqn, axis=0)\n",
    "std_smoothed_scores = np.std(best_smoothed_scores_dqn, axis=0)\n",
    "\n",
    "avg_dqn_completion_after = np.mean([dqn_completion_after[0],\n",
    "                                    dqn_completion_after[1],\n",
    "                                    dqn_completion_after[2],\n",
    "                                    dqn_completion_after[3],\n",
    "                                    dqn_completion_after[4]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dqn[0])), mean_smoothed_scores_dqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dqn, 97, axis=0), alpha=0.25)\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DQN_training.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSQN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'hebb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/luuk/Desktop/testing/training_q_learning.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(policy_net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env_name, policy_net, target_net, architecture, batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m               replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m               update_every, target_update_frequency, optimizer, learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m               num_episodes, max_steps, i_run, result_dir, seed, tau, spiking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m smoothed_scores, scores, best_average, best_average_after \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain_agent()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m np\u001b[39m.\u001b[39msave(result_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/scores_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i_run), scores)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/training_q_learning.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m np\u001b[39m.\u001b[39msave(result_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/smoothed_scores_DSQN_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i_run), smoothed_scores)\n",
      "File \u001b[0;32m~/Desktop/testing/src/dqn_agent.py:177\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/testing/src/dqn_agent.py:100\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(self, state, eps)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'hebb'"
     ]
    }
   ],
   "source": [
    "smoothed_scores_dsqn_all = []\n",
    "dsqn_completion_after = []\n",
    "\n",
    "for i_run in range(n_runs):\n",
    "    print(\"Run # {}\".format(i_run))\n",
    "    seed = int(seeds[i_run])\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    policy_net = FFNetwork(4, 64, 64, 2)\n",
    "    target_net = FFNetwork(4, 64, 64, 2)\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                  replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                  update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                  num_episodes, max_steps, i_run, result_dir, seed, tau, spiking=True)\n",
    "\n",
    "    smoothed_scores, scores, best_average, best_average_after = agent.train_agent()\n",
    "\n",
    "    np.save(result_dir + '/scores_{}'.format(i_run), scores)\n",
    "    np.save(result_dir + '/smoothed_scores_DSQN_{}'.format(i_run), smoothed_scores)\n",
    "\n",
    "    # save smoothed scores in list to plot later\n",
    "    smoothed_scores_dsqn_all.append(smoothed_scores)\n",
    "    dsqn_completion_after.append(best_average_after)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smoothed_scores_dsqn = [smoothed_scores_dsqn_all[0],\n",
    "                             smoothed_scores_dsqn_all[1],\n",
    "                             smoothed_scores_dsqn_all[2],\n",
    "                             smoothed_scores_dsqn_all[3],\n",
    "                             smoothed_scores_dsqn_all[4]]\n",
    "mean_smoothed_scores_dsqn = np.mean(best_smoothed_scores_dsqn, axis=0)\n",
    "\n",
    "avg_dsqn_completion_after = np.mean([dsqn_completion_after[0],\n",
    "                                    dsqn_completion_after[1],\n",
    "                                    dsqn_completion_after[2],\n",
    "                                    dsqn_completion_after[3],\n",
    "                                    dsqn_completion_after[4]])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(range(len(best_smoothed_scores_dsqn[0])), mean_smoothed_scores_dsqn)\n",
    "plt.fill_between(range(len(best_smoothed_scores_dsqn[0])),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 2, axis=0),\n",
    "                 np.nanpercentile(best_smoothed_scores_dsqn, 97, axis=0), alpha=0.25)\n",
    "\n",
    "plt.vlines(avg_dsqn_completion_after, 0, 250, 'C0')\n",
    "\n",
    "\n",
    "plt.ylim(0, 250)\n",
    "plt.grid(True)\n",
    "plt.savefig(result_dir + '/DSQN_training.png', dpi=1000)\n",
    "plt.title('CartPole-v0 DSQN')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
