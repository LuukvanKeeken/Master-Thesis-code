{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from BP_A2C.BP_A2C_agent import A2C_Agent, evaluate_BP_agent\n",
    "\n",
    "import site\n",
    "site.addsitedir('../src/')\n",
    "\n",
    "from backpropamine_A2C import BP_RNetwork\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'CartPole-v0'\n",
    "max_reward = 200\n",
    "max_steps = 200\n",
    "\n",
    "n_evaluations = 100\n",
    "num_fine_tuning_episodes = 250\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_episodes = 3000\n",
    "num_evaluation_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2C hyperparameters\n",
    "entropy_coef = 0.03 \n",
    "value_pred_coef = 0.1 \n",
    "gammaR = 0.99\n",
    "max_grad_norm = 4.0\n",
    "batch_size = 1\n",
    "print_every = 10\n",
    "save_every = 50\n",
    "selection_method = \"evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam hyperparameters\n",
    "learning_rate = 1e-4 # For Adam optimizer\n",
    "l2_coef = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_seeds = np.load('rstdp_cartpole_stuff/seeds/evaluation_seeds.npy')\n",
    "fine_tuning_seeds = np.load('rstdp_cartpole_stuff/seeds/rstdp_training_seeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory BP_A2C/fine_tuning_results/BP_A2C_RNN_fine_tuning_result_2_20231016 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('./BP_A2C/fine_tuning_results')\n",
    "if not any('fine_tuning_result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'fine_tuning_result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'BP_A2C/fine_tuning_results/BP_A2C_RNN_fine_tuning_result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model weights\n",
    "weights_0 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_0.pt', map_location=torch.device(device))\n",
    "weights_1 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_1.pt', map_location=torch.device(device))\n",
    "weights_2 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_2.pt', map_location=torch.device(device))\n",
    "weights_3 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_3.pt', map_location=torch.device(device))\n",
    "weights_4 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_4.pt', map_location=torch.device(device))\n",
    "weights = [weights_0, weights_1, weights_2, weights_3, weights_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE: 1.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 199.98\n",
      "model 3: 200.0\n",
      "model 4: 200.0\n",
      "PERCENTAGE: 1.2000000000000002\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 200.0\n",
      "PERCENTAGE: 1.3\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 200.0\n",
      "PERCENTAGE: 1.4000000000000001\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 200.0\n",
      "PERCENTAGE: 1.5\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 198.22\n",
      "PERCENTAGE: 1.6\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 153.3\n",
      "PERCENTAGE: 1.7000000000000002\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 30.2\n",
      "PERCENTAGE: 1.8\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 21.26\n",
      "PERCENTAGE: 1.9\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 21.47\n",
      "PERCENTAGE: 2.0\n",
      "model 0: 200.0\n",
      "model 1: 200.0\n",
      "model 2: 200.0\n",
      "model 3: 200.0\n",
      "model 4: 20.38\n"
     ]
    }
   ],
   "source": [
    "percentages = np.linspace(1.1, 2.0, 10)\n",
    "for percentage in percentages:\n",
    "    print(f\"PERCENTAGE: {percentage}\")\n",
    "    for i_run, w in enumerate(weights):\n",
    "# i_run = 2\n",
    "# w = weights[i_run]\n",
    "\n",
    "        agent_net = BP_RNetwork(4, 64, 2, 5).to(device)\n",
    "        agent_net.loadWeights(w)\n",
    "\n",
    "\n",
    "        # optimizer = torch.optim.Adam(agent_net.parameters(), lr = learning_rate)\n",
    "        # agent = A2C_Agent(env_name, 12, agent_net, entropy_coef, value_pred_coef, gammaR,\n",
    "        #                 max_grad_norm, max_steps, batch_size, num_training_episodes, optimizer, print_every,\n",
    "        #                 save_every, i_run, result_dir, selection_method, num_evaluation_episodes, evaluation_seeds, max_reward)\n",
    "\n",
    "        eval_rewards = evaluate_BP_agent(agent_net, env_name, n_evaluations, evaluation_seeds, percentage)\n",
    "        print(f\"model {i_run}: {np.mean(eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE: 1.1------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/Desktop/testing/.venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 199.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/Desktop/testing/.venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:    1 -- Reward:  200.00 -- Best reward:    -inf in episode    0\n",
      "Best individual stored after episode 1 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.2000000000000002------------\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.3------------\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.4000000000000001------------\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.5------------\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 198.22\n",
      "Episode:  121 -- Reward:  200.00 -- Best reward:  199.98 in episode  120\n",
      "Best individual stored after episode 121 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.6------------\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n",
      "Before fine-tuning: 200.0\n",
      "Maximum evaluation performance already reached before fine-tuning\n",
      "\n",
      "Best individual stored after episode 0 with reward 200.00\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(agent_net\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m agent \u001b[39m=\u001b[39m A2C_Agent(env_name, seed, agent_net, entropy_coef, value_pred_coef, gammaR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m               max_grad_norm, max_steps, batch_size, num_training_episodes, optimizer, print_every,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m               save_every, i_run, result_dir, selection_method, num_evaluation_episodes, evaluation_seeds, max_reward)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m eval_rewards \u001b[39m=\u001b[39m evaluate_BP_agent(agent\u001b[39m.\u001b[39;49magent_net, env_name, n_evaluations, evaluation_seeds, percentage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBefore fine-tuning: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(eval_rewards)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/fine_tuning_A2C.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m fine_tuned_weights, best_reward, best_episode \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mfine_tune_agent(num_fine_tuning_episodes, eval_skip, fine_tuning_seeds, percentage, n_evaluations, evaluation_seeds, max_reward)\n",
      "File \u001b[0;32m~/Desktop/testing/BP_A2C/BP_A2C_agent.py:314\u001b[0m, in \u001b[0;36mevaluate_BP_agent\u001b[0;34m(agent_net, env_name, num_episodes, evaluation_seeds, pole_length_modifier)\u001b[0m\n\u001b[1;32m    312\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(state)\n\u001b[1;32m    313\u001b[0m state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m#.to(device) #This as well?\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m policy_output, value, (hidden_activations, hebbian_traces) \u001b[39m=\u001b[39m agent_net\u001b[39m.\u001b[39;49mforward(state\u001b[39m.\u001b[39;49mfloat(), [hidden_activations, hebbian_traces])\n\u001b[1;32m    316\u001b[0m policy_dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(policy_output, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    318\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(policy_dist)\n",
      "File \u001b[0;32m~/Desktop/testing/backpropamine_A2C.py:57\u001b[0m, in \u001b[0;36mBP_RNetwork.forward\u001b[0;34m(self, inputs, hidden)\u001b[0m\n\u001b[1;32m     53\u001b[0m hebb \u001b[39m=\u001b[39m hidden[\u001b[39m1\u001b[39m]\n\u001b[1;32m     56\u001b[0m \u001b[39m# Each *column* of w, alpha and hebb contains the inputs weights to a single neuron\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m hactiv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mi2h(inputs) \u001b[39m+\u001b[39m hidden[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mbmm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha, hebb))\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)  )  \u001b[39m# Update the h-state\u001b[39;00m\n\u001b[1;32m     58\u001b[0m activout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh2o(hactiv)  \u001b[39m# Pure linear, raw scores - to be softmaxed later, outside the function\u001b[39;00m\n\u001b[1;32m     59\u001b[0m valueout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh2v(hactiv)\n",
      "File \u001b[0;32m~/Desktop/testing/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1252\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_is_full_backward_hook\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_full_backward_hook \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "percentages = np.linspace(1.1, 2.0, 10)\n",
    "seed = 59\n",
    "eval_skip = 1\n",
    "avg_best_rewards = []\n",
    "std_dev_best_rewards = []\n",
    "avg_best_episodes = []\n",
    "std_dev_best_episodes = []\n",
    "for percentage in percentages:\n",
    "    print(f\"PERCENTAGE: {percentage}------------\\n\")\n",
    "    best_rewards = []\n",
    "    best_episodes = []\n",
    "\n",
    "    # modified_env = gym.make(env_name)\n",
    "    # modified_env.unwrapped.length *= percentage\n",
    "    # weights_0 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_0.pt', map_location=torch.device(device))\n",
    "    # weights_1 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_1.pt', map_location=torch.device(device))\n",
    "    # weights_2 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_2.pt', map_location=torch.device(device))\n",
    "    # weights_3 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_3.pt', map_location=torch.device(device))\n",
    "    # weights_4 = torch.load('BP_A2C/results/a2c_result_2_20231014_entropycoef_0.03_valuepredcoef_0.1_batchsize_128_maxsteps_200_maxgradnorm_4.0_gammaR_0.99_l2coef_0_learningrate_0.0001_numtrainepisodes_3000_selectionmethod_evaluation/checkpoint_BP_A2C_4.pt', map_location=torch.device(device))\n",
    "    # weights = [weights_0, weights_1, weights_2, weights_3, weights_4]\n",
    "    for i_run, w in enumerate(weights):\n",
    "\n",
    "        agent_net = BP_RNetwork(4, 64, 2, 5).to(device)\n",
    "        agent_net.loadWeights(w)\n",
    "        \n",
    "\n",
    "        optimizer = torch.optim.Adam(agent_net.parameters(), lr = learning_rate)\n",
    "        agent = A2C_Agent(env_name, seed, agent_net, entropy_coef, value_pred_coef, gammaR,\n",
    "                      max_grad_norm, max_steps, batch_size, num_training_episodes, optimizer, print_every,\n",
    "                      save_every, i_run, result_dir, selection_method, num_evaluation_episodes, evaluation_seeds, max_reward)\n",
    "\n",
    "        \n",
    "        eval_rewards = evaluate_BP_agent(agent.agent_net, env_name, n_evaluations, evaluation_seeds, percentage)\n",
    "        print(f\"Before fine-tuning: {np.mean(eval_rewards)}\")\n",
    "        \n",
    "\n",
    "\n",
    "        fine_tuned_weights, best_reward, best_episode = agent.fine_tune_agent(num_fine_tuning_episodes, eval_skip, fine_tuning_seeds, percentage, n_evaluations, evaluation_seeds, max_reward)\n",
    "\n",
    "        best_rewards.append(best_reward)\n",
    "        best_episodes.append(best_episode)\n",
    "\n",
    "            \n",
    "    avg_best_rewards.append(np.mean(best_rewards))\n",
    "    std_dev_best_rewards.append(np.std(best_rewards))\n",
    "    avg_best_episodes.append(np.mean(best_episodes))\n",
    "    std_dev_best_episodes.append(np.std(best_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.2, 0.0, 0.0, 0.0, 24.2, 24.0, 24.0, 24.0, 23.8, 23.8]\n",
      "[0.4000000000000001, 0.0, 0.0, 0.0, 48.4, 48.0, 48.0, 48.0, 47.6, 47.6]\n"
     ]
    }
   ],
   "source": [
    "print(avg_best_rewards)\n",
    "print(std_dev_best_rewards)\n",
    "print(avg_best_episodes)\n",
    "print(std_dev_best_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4000000000000001, 0.0, 0.0, 0.0, 48.4, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(f\"{result_dir}/best_rewards_correct.npy\", [avg_best_rewards, std_dev_best_rewards])\n",
    "np.save(f\"{result_dir}/best_episodes_correct.npy\", [avg_best_episodes, std_dev_best_episodes])\n",
    "[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
    "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "[0.2, 0.0, 0.0, 0.0, 24.2, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "[0.4000000000000001, 0.0, 0.0, 0.0, 48.4, 0.0, 0.0, 0.0, 0.0, 0.0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
