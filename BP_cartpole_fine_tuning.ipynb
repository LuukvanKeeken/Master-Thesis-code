{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from cartpole_stuff.src.utils import evaluate_policy, rstdp_train_cartpole, evaluate_BP_policy\n",
    "from cartpole_stuff.src.dqn_agent import Agent, ReplayBuffer\n",
    "\n",
    "import site\n",
    "site.addsitedir('../src/')\n",
    "\n",
    "from cartpole_stuff.src.dsnn import RSTDPNet\n",
    "from backpropamine_DQN import BP_RNetwork, Standard_RNetwork\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'CartPole-v0'\n",
    "max_reward = 200\n",
    "max_steps = 200\n",
    "\n",
    "n_evaluations = 100\n",
    "num_fine_tuning_episodes = 250\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory BP_fine_tuning_result_5_20231010 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('fine_tuning_result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'fine_tuning_result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'BP_fine_tuning_result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 0.999\n",
    "update_every = 4\n",
    "target_update_frequency = 100\n",
    "learning_rate = 0.001 # lr is 0.0001 for simple maze as default\n",
    "l2_coef = 0 # 0 is default in simple maze task\n",
    "replay_memory_size = 4*10**4\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments\n",
    "original_env = gym.make(env_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN Hyperparameters\n",
    "simulation_time = 8\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "threshold = 0.5\n",
    "weight_scale = 1\n",
    "architecture = [4, 64, 64, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_seeds = np.load('cartpole_stuff/seeds/evaluation_seeds.npy')\n",
    "fine_tuning_seeds = np.load('cartpole_stuff/seeds/rstdp_training_seeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model weights\n",
    "weights_0 = torch.load('simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_0.pt', map_location=torch.device(device))\n",
    "weights_1 = torch.load('simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_1.pt', map_location=torch.device(device))\n",
    "weights_2 = torch.load('simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_2.pt', map_location=torch.device(device))\n",
    "weights_3 = torch.load('simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_3.pt', map_location=torch.device(device))\n",
    "weights_4 = torch.load('simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_4.pt', map_location=torch.device(device))\n",
    "weights = [weights_0, weights_1, weights_2, weights_3, weights_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE: 1.1------------\n",
      "\n",
      "Episode:    7 -- Reward:  200.00 -- Best reward:  156.01 in episode    1\n",
      "Best individual stored after episode 7 with reward 200.00\n",
      "\n",
      "Episode:    3 -- Reward:  200.00 -- Best reward:  172.12 in episode    1\n",
      "Best individual stored after episode 3 with reward 200.00\n",
      "\n",
      "Episode:    1 -- Reward:  200.00 -- Best reward:    -inf in episode   -1\n",
      "Best individual stored after episode 1 with reward 200.00\n",
      "\n",
      "Episode:    2 -- Reward:  200.00 -- Best reward:  115.00 in episode    1\n",
      "Best individual stored after episode 2 with reward 200.00\n",
      "\n",
      "Episode:   14 -- Reward:  200.00 -- Best reward:  157.76 in episode   13\n",
      "Best individual stored after episode 14 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.5------------\n",
      "\n",
      "Episode:  100 -- Reward:   64.70 -- Best reward:  167.83 in episode   33\n",
      "Best individual stored after episode 33 with reward 167.83\n",
      "\n",
      "Episode:   24 -- Reward:  200.00 -- Best reward:  198.15 in episode   17\n",
      "Best individual stored after episode 24 with reward 200.00\n",
      "\n",
      "Episode:  100 -- Reward:   72.91 -- Best reward:  148.26 in episode   92\n",
      "Best individual stored after episode 92 with reward 148.26\n",
      "\n",
      "Episode:    3 -- Reward:  200.00 -- Best reward:   58.16 in episode    2\n",
      "Best individual stored after episode 3 with reward 200.00\n",
      "\n",
      "Episode:   11 -- Reward:  200.00 -- Best reward:  199.66 in episode    9\n",
      "Best individual stored after episode 11 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 2.0------------\n",
      "\n",
      "Episode:   97 -- Reward:  200.00 -- Best reward:  197.62 in episode   95\n",
      "Best individual stored after episode 97 with reward 200.00\n",
      "\n",
      "Episode:   31 -- Reward:  200.00 -- Best reward:  181.77 in episode   30\n",
      "Best individual stored after episode 31 with reward 200.00\n",
      "\n",
      "Episode:  100 -- Reward:   76.04 -- Best reward:  109.96 in episode   12\n",
      "Best individual stored after episode 12 with reward 109.96\n",
      "\n",
      "Episode:   72 -- Reward:  200.00 -- Best reward:  199.86 in episode   15\n",
      "Best individual stored after episode 72 with reward 200.00\n",
      "\n",
      "Episode:   10 -- Reward:  200.00 -- Best reward:  175.36 in episode    5\n",
      "Best individual stored after episode 10 with reward 200.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BP_finetuning_rewards = []\n",
    "adapted_weights_collection = []\n",
    "percentages = np.linspace(1.1, 2.0, 10)\n",
    "# percentages = [1.1, 1.5, 2.0]\n",
    "seed = 59\n",
    "eval_skip = 1\n",
    "avg_best_rewards = []\n",
    "avg_best_episodes = []\n",
    "for percentage in percentages:\n",
    "    print(f\"PERCENTAGE: {percentage}------------\\n\")\n",
    "    best_rewards = []\n",
    "    best_episodes = []\n",
    "\n",
    "    modified_env = gym.make(env_name)\n",
    "    modified_env.unwrapped.length *= percentage\n",
    "\n",
    "    for i_run, w in enumerate(weights):\n",
    "\n",
    "        policy_net = BP_RNetwork(4, 64, 2, 5).to(device)\n",
    "        policy_net.loadWeights(w)\n",
    "        target_net = BP_RNetwork(4, 64, 2, 5).to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr = learning_rate, weight_decay = l2_coef) \n",
    "        agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                    replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                    update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                    num_episodes, max_steps, i_run, result_dir, seed, tau)\n",
    "            \n",
    "        fine_tuned_weights, best_reward, best_episode = agent.fine_tune_agent(num_fine_tuning_episodes, eval_skip, fine_tuning_seeds, modified_env, n_evaluations, evaluation_seeds, max_reward)\n",
    "\n",
    "        best_rewards.append(best_reward)\n",
    "        best_episodes.append(best_episode)\n",
    "\n",
    "            \n",
    "            # BP_finetuning_rewards.append(rewards)\n",
    "            # adapted_weights_collection.append(adapted_weights)\n",
    "    avg_best_rewards.append(np.mean(best_rewards))\n",
    "    avg_best_episodes.append(np.mean(best_episodes))\n",
    "        \n",
    "    # adapted_weights_collection = [(list(aw.values()), []) for aw in adapted_weights_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.0, 197.326, 187.166]\n",
      "[7.0, 17.4, 29.0]\n"
     ]
    }
   ],
   "source": [
    "print(avg_best_rewards)\n",
    "print(avg_best_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
