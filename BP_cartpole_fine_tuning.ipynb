{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "\n",
    "from rstdp_cartpole_stuff.src.dqn_agent import Agent, ReplayBuffer\n",
    "\n",
    "import site\n",
    "site.addsitedir('../src/')\n",
    "\n",
    "\n",
    "from backpropamine_DQN import BP_RNetwork, Standard_RNetwork\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "env_name = 'CartPole-v0'\n",
    "max_reward = 200\n",
    "max_steps = 200\n",
    "\n",
    "n_evaluations = 100\n",
    "num_fine_tuning_episodes = 250\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Directory BP_fine_tuning_result_5_20231016 to store the results in\n"
     ]
    }
   ],
   "source": [
    "# Create Results Directory\n",
    "dirs = os.listdir('.')\n",
    "if not any('fine_tuning_result' in d for d in dirs):\n",
    "    result_id = 1\n",
    "else:\n",
    "    results = [d for d in dirs if 'fine_tuning_result' in d]\n",
    "    result_id = len(results) + 1\n",
    "\n",
    "# Get today's date and add it to the results directory\n",
    "d = date.today()\n",
    "result_dir = 'BP_fine_tuning_result_' + str(result_id) + '_{}'.format(\n",
    "    str(d.year) + str(d.month) + str(d.day))\n",
    "os.mkdir(result_dir)\n",
    "print('Created Directory {} to store the results in'.format(result_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "discount_factor = 0.999\n",
    "eps_start = 1.0\n",
    "eps_end = 0.05\n",
    "eps_decay = 0.999\n",
    "update_every = 4\n",
    "target_update_frequency = 100\n",
    "learning_rate = 0.001 # lr is 0.0001 for simple maze as default\n",
    "l2_coef = 0 # 0 is default in simple maze task\n",
    "replay_memory_size = 4*10**4\n",
    "tau = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luuk/Desktop/testing/.venv/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Create environments\n",
    "original_env = gym.make(env_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNN Hyperparameters\n",
    "simulation_time = 8\n",
    "alpha = 0.8\n",
    "beta = 0.8\n",
    "threshold = 0.5\n",
    "weight_scale = 1\n",
    "architecture = [4, 64, 64, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_seeds = np.load('rstdp_cartpole_stuff/seeds/evaluation_seeds.npy')\n",
    "fine_tuning_seeds = np.load('rstdp_cartpole_stuff/seeds/rstdp_training_seeds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model weights\n",
    "weights_0 = torch.load('trained_models/simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_0.pt', map_location=torch.device(device))\n",
    "weights_1 = torch.load('trained_models/simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_1.pt', map_location=torch.device(device))\n",
    "weights_2 = torch.load('trained_models/simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_2.pt', map_location=torch.device(device))\n",
    "weights_3 = torch.load('trained_models/simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_3.pt', map_location=torch.device(device))\n",
    "weights_4 = torch.load('trained_models/simple_BP_DQRNN_training_1000eps/checkpoint_BP_DQRNN_4.pt', map_location=torch.device(device))\n",
    "weights = [weights_0, weights_1, weights_2, weights_3, weights_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE: 1.1------------\n",
      "\n",
      "Episode:    7 -- Reward:  200.00 -- Best reward:  156.01 in episode    1\n",
      "Best individual stored after episode 7 with reward 200.00\n",
      "\n",
      "Episode:    3 -- Reward:  200.00 -- Best reward:  172.12 in episode    1\n",
      "Best individual stored after episode 3 with reward 200.00\n",
      "\n",
      "Episode:    1 -- Reward:  200.00 -- Best reward:    -inf in episode   -1\n",
      "Best individual stored after episode 1 with reward 200.00\n",
      "\n",
      "Episode:    2 -- Reward:  200.00 -- Best reward:  115.00 in episode    1\n",
      "Best individual stored after episode 2 with reward 200.00\n",
      "\n",
      "Episode:   14 -- Reward:  200.00 -- Best reward:  157.76 in episode   13\n",
      "Best individual stored after episode 14 with reward 200.00\n",
      "\n",
      "PERCENTAGE: 1.2000000000000002------------\n",
      "\n",
      "Episode:   42 -- Reward:  200.00 -- Best reward:  194.45 in episode   40\n",
      "Best individual stored after episode 42 with reward 200.00\n",
      "\n",
      "Episode:    1 -- Reward:  150.83 -- Best reward:    -inf in episode   -1\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(policy_net\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m learning_rate, weight_decay \u001b[39m=\u001b[39m l2_coef) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env_name, policy_net, target_net, architecture, batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             update_every, target_update_frequency, optimizer, learning_rate,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m             num_episodes, max_steps, i_run, result_dir, seed, tau)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m fine_tuned_weights, best_reward, best_episode \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mfine_tune_agent(num_fine_tuning_episodes, eval_skip, fine_tuning_seeds, modified_env, n_evaluations, evaluation_seeds, max_reward)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m best_rewards\u001b[39m.\u001b[39mappend(best_reward)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luuk/Desktop/testing/BP_cartpole_fine_tuning.ipynb#X12sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m best_episodes\u001b[39m.\u001b[39mappend(best_episode)\n",
      "File \u001b[0;32m~/Desktop/testing/rstdp_cartpole_stuff/src/dqn_agent.py:271\u001b[0m, in \u001b[0;36mAgent.fine_tune_agent\u001b[0;34m(self, fine_tuning_episodes, eval_skip, fine_tuning_seeds, modified_env, n_evaluations, evaluation_seeds, max_reward)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m (i_episode \u001b[39m%\u001b[39m eval_skip \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m     eval_rewards \u001b[39m=\u001b[39m evaluate_BP_policy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, n_evaluations, evaluation_seeds)\n\u001b[1;32m    272\u001b[0m     avg_eval_reward \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(eval_rewards)\n\u001b[1;32m    274\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpisode: \u001b[39m\u001b[39m{:4d}\u001b[39;00m\u001b[39m -- Reward: \u001b[39m\u001b[39m{:7.2f}\u001b[39;00m\u001b[39m -- Best reward: \u001b[39m\u001b[39m{:7.2f}\u001b[39;00m\u001b[39m in episode \u001b[39m\u001b[39m{:4d}\u001b[39;00m\u001b[39m\"\u001b[39m\\\n\u001b[1;32m    275\u001b[0m         \u001b[39m.\u001b[39mformat(i_episode, avg_eval_reward, best_reward, best_episode), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)    \n",
      "File \u001b[0;32m~/Desktop/testing/rstdp_cartpole_stuff/src/utils.py:72\u001b[0m, in \u001b[0;36mevaluate_BP_policy\u001b[0;34m(policy_net, env, n_evaluations, seeds)\u001b[0m\n\u001b[1;32m     70\u001b[0m     hidden_activations, hebbian_traces \u001b[39m=\u001b[39m hidden\n\u001b[1;32m     71\u001b[0m     action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(mem_result)\n\u001b[0;32m---> 72\u001b[0m     state, r, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     73\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m     74\u001b[0m eval_rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/Desktop/testing/.venv/lib/python3.8/site-packages/gym/wrappers/time_limit.py:16\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     18\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Desktop/testing/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:130\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_beyond_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49marray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate), reward, done, {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "percentages = np.linspace(1.1, 2.0, 10)\n",
    "seed = 59\n",
    "eval_skip = 1\n",
    "avg_best_rewards = []\n",
    "std_dev_best_rewards = []\n",
    "avg_best_episodes = []\n",
    "std_dev_best_episodes = []\n",
    "for percentage in percentages:\n",
    "    print(f\"PERCENTAGE: {percentage}------------\\n\")\n",
    "    best_rewards = []\n",
    "    best_episodes = []\n",
    "\n",
    "    modified_env = gym.make(env_name)\n",
    "    modified_env.unwrapped.length *= percentage\n",
    "    \n",
    "    for i_run, w in enumerate(weights):\n",
    "\n",
    "        policy_net = BP_RNetwork(4, 64, 2, 5).to(device)\n",
    "        policy_net.loadWeights(w)\n",
    "        target_net = BP_RNetwork(4, 64, 2, 5).to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr = learning_rate, weight_decay = l2_coef) \n",
    "        agent = Agent(env_name, policy_net, target_net, architecture, batch_size,\n",
    "                    replay_memory_size, discount_factor, eps_start, eps_end, eps_decay,\n",
    "                    update_every, target_update_frequency, optimizer, learning_rate,\n",
    "                    num_episodes, max_steps, i_run, result_dir, seed, tau)\n",
    "            \n",
    "        fine_tuned_weights, best_reward, best_episode = agent.fine_tune_agent(num_fine_tuning_episodes, eval_skip, fine_tuning_seeds, modified_env, n_evaluations, evaluation_seeds, max_reward)\n",
    "\n",
    "        best_rewards.append(best_reward)\n",
    "        best_episodes.append(best_episode)\n",
    "\n",
    "            \n",
    "    avg_best_rewards.append(np.mean(best_rewards))\n",
    "    std_dev_best_rewards.append(np.std(best_rewards))\n",
    "    avg_best_episodes.append(np.mean(best_episodes))\n",
    "    std_dev_best_episodes.append(np.std(best_episodes))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.0, 200.0, 200.0, 200.0, 190.478, 199.02599999999998, 186.132, 192.072, 162.158, 165.594]\n",
      "[0.0, 0.0, 0.0, 0.0, 19.044000000000004, 1.5233857029655964, 26.669124020109848, 9.949157552275466, 46.67789301157455, 43.52876363969002]\n",
      "[5.4, 31.2, 79.8, 75.8, 103.6, 112.2, 112.8, 157.0, 133.4, 120.8]\n",
      "[4.758150901348127, 30.87005021051958, 77.4477888644989, 75.13028683560313, 98.17046398993945, 93.97318766541869, 74.46985967490473, 60.64981450919698, 57.37804458152961, 54.590841722765184]\n"
     ]
    }
   ],
   "source": [
    "print(avg_best_rewards)\n",
    "print(std_dev_best_rewards)\n",
    "print(avg_best_episodes)\n",
    "print(std_dev_best_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"fine_tuning_results/BP_DQRNN_1000eps_best_rewards_correct.npy\", [avg_best_rewards, std_dev_best_rewards])\n",
    "np.save(\"fine_tuning_results/BP_DQRNN_1000eps_best_episodes_correct.npy\", [avg_best_episodes, std_dev_best_episodes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
